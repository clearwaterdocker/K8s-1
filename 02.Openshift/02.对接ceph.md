# 对接ceph

kubernetes原生支持ceph rbd的dynamic provisioning特性，通过以下步骤，即可对接openshift和ceph rbd存储，实现存储的自动化管理。

## 配置ceph

```bash
[root@ceph01 ~]# ceph osd pool create kube 256
pool 'kube' created
[root@ceph01 ~]# ceph osd pool application enable kube kubernetes
enabled application 'kubernetes' on pool 'kube'
```

* 创建rbd池kube；

```bash
[root@ceph01 ~]# ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube'
[root@ceph01 ~]# ceph auth get-key client.kube | base64
QVFBSGpxeGJISCt6S0JBQUlHK2toSnVBWHdyZC9VZnJxOVhLVUE9PQ==
```

* 配置client.kube ceph用户，并授权;
* 获取client.kube的key，并使用base64编码，用于配置openshift的secret；

```bash
[root@ceph01 ~]# ceph auth get-key client.admin | base64
QVFBMGJheGI3S25zSGhBQWJFbFlCcnB6OE1jZWppSjRkYUN6clE9PQ==
```

* 获取client.admin的key，并使用base64编码，用于配置openshift的secret；

## 配置openshift

```bash
[root@okd01 ~]# vi ceph-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
  namespace: kube-system
data:
  key: QVFBMGJheGI3S25zSGhBQWJFbFlCcnB6OE1jZWppSjRkYUN6clE9PQ==
type: kubernetes.io/rbd
[root@okd01 ~]# oc create -f ceph-secret.yaml
secret "ceph-secret" created
[root@okd01 ~]# oc get secret ceph-secret -n kube-system
NAME          TYPE                DATA      AGE
ceph-secret   kubernetes.io/rbd   1         1m
```

* 配置client.admin用户的secret，key字段为以上获取的client.admin并通过base64编码的字符串；

```bash
[root@okd01 ~]# vi ceph-secret-user.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-user
  namespace: kube-system
data:
  key: QVFBSGpxeGJISCt6S0JBQUlHK2toSnVBWHdyZC9VZnJxOVhLVUE9PQ==
type: kubernetes.io/rbd
[root@okd01 ~]# oc create -f ceph-secret-user.yaml
secret "ceph-secret-user" created
```

* 配置client.kube用户的secret，key字段为以上获取的client.admin并通过base64编码的字符串；

```bash
[root@okd01 ~]# vi ceph-rbd-sc.yaml
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: ceph-rbd
  annotations:
     storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.16.16.240:6789,172.16.16.241:6789,172.16.16.242:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: kube
  userId: kube
  userSecretName: ceph-secret-user
  fsType: ext4
  imageFormat: "2"
  imageFeatures: "layering"
[root@okd01 ~]# oc create -f ceph-rbd-sc.yaml
```

* 配置storageclass；
* `storageclass.beta.kubernetes.io/is-default-class: "true"`将ceph-rbd设置为默认存储；
* `monitors: 172.16.16.240:6789,172.16.16.241:6789,172.16.16.242:6789`指定所有的mon，提供高可用；
* `ceph-secret-user`和`ceph-secret`需要在同一namespace下。

## 测试自动分配

```bash
apiVersion: v1
kind: Pod
metadata:
  name: ceph-pod1
spec:
  containers:
  - name: ceph-busybox
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: ceph-vol1
      mountPath: /usr/share/busybox
      readOnly: false
  volumes:
  - name: ceph-vol1
    persistentVolumeClaim:
      claimName: ceph-claim
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-claim
spec:
  accessModes:  
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```

* 创建一个busybox pod来测试存储的自动分配管理；

```bash
[root@okd01 ~]# kubectl exec  -n kube-system ceph-pod1 df
Filesystem           1K-blocks      Used Available Use% Mounted on
overlay               61831684   4095060  57736624   7% /
tmpfs                  8216076         0   8216076   0% /dev
tmpfs                  8216076         0   8216076   0% /sys/fs/cgroup
/dev/sda2             61831684   4095060  57736624   7% /dev/termination-log
/dev/sda2             61831684   4095060  57736624   7% /run/secrets
/dev/sda2             61831684   4095060  57736624   7% /etc/resolv.conf
/dev/sda2             61831684   4095060  57736624   7% /etc/hostname
/dev/sda2             61831684   4095060  57736624   7% /etc/hosts
shm                      65536         0     65536   0% /dev/shm
/dev/rbd0              1998672      6144   1871288   0% /usr/share/busybox
tmpfs                  8216076        16   8216060   0% /var/run/secrets/kubernetes.io/serviceaccount
tmpfs                  8216076         0   8216076   0% /proc/acpi
tmpfs                  8216076         0   8216076   0% /proc/kcore
tmpfs                  8216076         0   8216076   0% /proc/timer_list
tmpfs                  8216076         0   8216076   0% /proc/timer_stats
tmpfs                  8216076         0   8216076   0% /proc/sched_debug
tmpfs                  8216076         0   8216076   0% /proc/scsi
tmpfs                  8216076         0   8216076   0% /sys/firmware
```

## 修改project默认模版

由于ceph-secret和ceph-secret-user都在kube-system namespace，其他namespace使用自动分配，由于namespace中没有ceph-secret-user，因此无法挂载ceph rbd。需要修改project模版，在创建project时默认创建ceph-secret-user。

```bash
[root@okd01 ~]# oc adm create-bootstrap-project-template -o yaml > template.yaml
[root@okd01 ~]# vi template.yaml
apiVersion: v1
kind: Template
metadata:
  creationTimestamp: null
  name: project-request
objects:
- apiVersion: project.openshift.io/v1
  kind: Project
  metadata:
    annotations:
      openshift.io/description: ${PROJECT_DESCRIPTION}
      openshift.io/display-name: ${PROJECT_DISPLAYNAME}
      openshift.io/requester: ${PROJECT_REQUESTING_USER}
    creationTimestamp: null
    name: ${PROJECT_NAME}
  spec: {}
  status: {}
- apiVersion: v1
  kind: Secret
  metadata:
    name: ceph-secret-user
  data:
    key:  QVFBSGpxeGJISCt6S0JBQUlHK2toSnVBWHdyZC9VZnJxOVhLVUE9PQ==
  type:
    kubernetes.io/rbd
```

* 在project的template定义段，加入ceph-secret-user；

```bash
[root@okd01 ~]# oc create -f template.yaml -n default
template.template.openshift.io "project-request" created
```

* 创建模版；

```bash
[root@okd01 ~]# vi /etc/origin/master/master-config.yaml
projectConfig:
  defaultNodeSelector: node-role.kubernetes.io/compute=true
  projectRequestMessage: ''
  projectRequestTemplate: 'default/project-request'
[root@okd01 ~]# systemctl restart origin-node
```

* 修改openshift master配置文件，将默认模版指向新建的模版；
* 重启master，重新加载配置文件；

```bash
[root@okd01 ~]# kubectl exec  -n dev ceph-pod1 df
Filesystem           1K-blocks      Used Available Use% Mounted on
overlay               61831684   4116424  57715260   7% /
tmpfs                  8216076         0   8216076   0% /dev
tmpfs                  8216076         0   8216076   0% /sys/fs/cgroup
/dev/sda2             61831684   4116424  57715260   7% /dev/termination-log
/dev/sda2             61831684   4116424  57715260   7% /run/secrets
/dev/sda2             61831684   4116424  57715260   7% /etc/resolv.conf
/dev/sda2             61831684   4116424  57715260   7% /etc/hostname
/dev/sda2             61831684   4116424  57715260   7% /etc/hosts
shm                      65536         0     65536   0% /dev/shm
/dev/rbd0              1998672      6144   1871288   0% /usr/share/busybox
tmpfs                  8216076        16   8216060   0% /var/run/secrets/kubernetes.io/serviceaccount
tmpfs                  8216076         0   8216076   0% /proc/acpi
tmpfs                  8216076         0   8216076   0% /proc/kcore
tmpfs                  8216076         0   8216076   0% /proc/timer_list
tmpfs                  8216076         0   8216076   0% /proc/timer_stats
tmpfs                  8216076         0   8216076   0% /proc/sched_debug
tmpfs                  8216076         0   8216076   0% /proc/scsi
tmpfs                  8216076         0   8216076   0% /sys/firmware
```

* 重新创建`dev`project，使用busybox来测试pod挂载rbd；

## 问题

```bash
MountVolume.WaitForAttach failed for volume "pvc-a1e3f6c9-c23e-11e8-8fd2-005056a3191a" : fail to check rbd image status with: (exit status 1), rbd output: (2018-09-27 18:18:50.337675 7f49b6ed37c0 -1 did not load config file, using default settings. 2018-09-27 18:18:50.383495 7f49b6ed1700 0 -- 172.16.16.84:0/1003560 >> 172.16.16.242:6789/0 pipe(0x3718b80 sd=3 :46766 s=1 pgs=0 cs=0 l=1 c=0x371ce70).connect protocol feature mismatch, my 83ffffffffffff < peer 481dff8eea4fffb missing 400000000000000.....

[root@ceph01 ~]# ceph osd crush tunables hammer
adjusted tunables profile to hammer
```

* okd和ceph服务器操作系统`CentOS-7.5-1804`内核版本`3.10.0-862.11.6.el7.x86_64`，在pod挂载rbd时，出现以上错误，和ceph的crush tunables特性有关，RH建议使用低版本的crush tunables profile。参考链接如下：

[What does the error message "feature set mismatch missing 400000000000000" mean, in Red Hat Ceph Storage?](https://access.redhat.com/solutions/2591751)